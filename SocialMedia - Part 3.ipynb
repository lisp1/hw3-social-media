{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <table><tr><td><img src=\"images/dbmi_logo.png\" width=\"75\" height=\"73\" alt=\"Pitt Biomedical Informatics logo\"></td><td><img src=\"images/pitt_logo.png\" width=\"75\" height=\"75\" alt=\"University of Pittsburgh logo\"></td></tr></table>\n",
    " \n",
    "\n",
    "# Social Media and Data Science - Part 3\n",
    "\n",
    "Data science modules developed by the University of Pittsburgh Biomedical Informatics Training Program with the support of the National Library of Medicine data science supplement to the University of Pittsburgh (Grant # T15LM007059-30S1). \n",
    "\n",
    "Developed by Harry Hochheiser, harryh@pitt.edu. All errors are my responsibility.\n",
    "\n",
    "Done by Pengan Li, pel85@pitt.edu. Feel free to contact me if you have any question or suggestion about my answers.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *Goal*: Use social media posts to explore the appplication of text and natural language processing to see what might be learned from online interactions.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as depression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Introduction\n",
    "\n",
    "This module continues the Social Media Data Science module started in [Part 1](SocialMedia%20-%20Part%201.ipynb) and [Part 2](SocialMedia%20-%20Part%202.ipynb), covering the natural language processing analysis of our tweet corpus, providing an introduction to basic concepts of Natural Language Processing.\n",
    "  \n",
    "Our case study will apply these topics to Twitter discussions of smoking and vaping. Although details of the tools used to access data and the format and content of the data may differ for various services, the strategies and procedures used to analyze the data will generalize to other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0.1 Setup\n",
    "\n",
    "Before we dig in, we must grab a bit of code from [Part 1](SocialMedia%20-%20Part%201.ipynb)and [Part 2](SocialMedia%20-%20Part%202.ipynb):\n",
    "\n",
    "1. Our Tweets class\n",
    "3. Our twitter API Keys - be sure to copy the keys that you generated when you completed [Part 1](SocialMedia%20-%20Part%201.ipynb).\n",
    "4. Configuration of our Twitter connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",tweet_mode='extended',count=corpus_size)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(30)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def combineTweets(self,other):\n",
    "        for otherid in other.getIds():\n",
    "            tweet = other.getTweet(otherid)\n",
    "            searchTerm = other.getSearchTerm(otherid)\n",
    "            searchTime = other.getSearchTime(otherid)\n",
    "            self.addTweet(tweet,searchTime,searchTerm)\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' in tweet:\n",
    "            return tweet['codes']\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary\n",
    "\n",
    "    #new functions\n",
    "    \n",
    "    def clearcode(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        tweet['codes'].clear()\n",
    "        \n",
    "    def removecode(self,id,code):\n",
    "        tweet=self.getTweet(id);\n",
    "        if code in tweet['codes']:\n",
    "            tweet['codes'].remove(code)\n",
    "        \n",
    "    def freq(self):\n",
    "        codes={};\n",
    "        for id in self.tweets.keys():\n",
    "            code=self.getCodes(id);\n",
    "            if code!=None:\n",
    "                for ele in code:\n",
    "                    if ele not in codes:\n",
    "                        codes[ele]=1;\n",
    "                    else:\n",
    "                        codes[ele]=codes[ele]+1;\n",
    "        codes=sorted(codes.items(),key = lambda x:x[1]);\n",
    "        return codes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the values of your keys into these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Natural langauge processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to build a classifier capable of distinguishing tweets related to tobacco smoing from other, unrelated tweets. To do this, we will use ome basic natural language processing to explore the types of words and language found in the tweets. \n",
    "\n",
    "To do this, we will use the [spaCy](https://spaCy.io/) Python NLP package. spaCy provides significant NLP power out-of-the box, with customization facilities offering greater flexibility at various stages of the pipeline. Details can be found at the  [spaCy web site](https://spaCy.io/), and in this [tutorial](https://nicschrading.com/project/Intro-to-NLP-with-spaCy/). spaCy is built on a neural network model based on recent developments in NLP research. See the [spaCy architecture](https://spaCy.io/api/) description for an overview.\n",
    "\n",
    "Before we get into the deails, a bit of an introduction. \n",
    "\n",
    "Natural Language Processing involves a series of operations on an input text, each building off of the previous step to add additional insight and undertanding.  Thus, many NLP packages run as pipeline processors providing modular components at each stage of the process. Separating key steps into discrete packages provides needed modularity, as developers can modify and customize individual components as needed. spaCy, like other NLP tools including [GATE](https://gate.ac.uk/) and [cTAKES](https://ctaes.apache.org)  operate on such a model. Although the specific components of each pipeline vary from system to system (and from tasks to task, the key tasks are roughly similar:\n",
    "\n",
    "1. *Tokenizing*: splitting the text into words, punctuation, and other markers.\n",
    "2. *Part of speech tagging*: Classifying terms as nouns, verbs, adjective, adverbs, ec.\n",
    "3. *Dependency Parsing* or *Chunking*: Defining relationships between tokens (subject and object of sentence) and grouping into noun and veb phrases.\n",
    "4. *Named Entity Recognition*: Mapping words or phrases to standard vocabularies or other common, known values. This step is often key for linking free text to accepted terms for diseases, symptoms, and/or anatomic locations.\n",
    "\n",
    "Each of these steps might be accomplished through rules, machine learning models, or some combination of approaches. After these initial steps are complete, results might be used to identify relationships between items in the text, build classifiers, or otherwise conduct further analysis. We'll get into these topics later.\n",
    "\n",
    "The [spaCy documentation](https://spaCy.io/usage/spaCy-101) and [cTAKES default pipeline description](https://cwiki.apache.org/confluence/display/CTAKES/Default+Clinical+Pipeline) provide two examples of how these components might be arranged in practice.  For more information on NLP theory and methods, see [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/), perhaps the leading NLP textbook.\n",
    "\n",
    "Given this introduction, we can read in our tweets and get to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 Reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the 'smoking' tweets that you created in [Part 1](SocialMedia%20-%20Part%201.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go along with these tweets, let's search for, and save, a set of tweets for the search term 'vaping':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaping = Tweets(\"vaping\",100)\n",
    "vaping.saveTweets(\"tweets-vaping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2 NLP Roadmap\n",
    "\n",
    "\n",
    "spaCy, like many other natural laguage processing tools, operates as a *pipeline* - a sequential series of operations, each of which conducts some analysis and passes results on to the next.  Each of the steps on the pipeline can operate both on the original text and on any of the results of the previous stages. The basic Spacy pipeline starts with the following steps:\n",
    "\n",
    "1. Tokenizing - splitting into individual elements.\n",
    "2. Tagging - assigning part-of-speech tags\n",
    "3. Parsing - identifying relaionships between elements of a sentence.\n",
    "4. Named Entity Recogntion (NER) - identifying domain-specific nounds and concepts. In biomedical literature, this might mean diseases, symptoms, anatomic locations, etc. \n",
    "\n",
    "Tokenizing is the assumed first stage of every pipeline. To see the contents of a pipeline, we can create an NLP object for the English language and iterate over the components of the pipeline. Although we'll usually use all of the components of the pipeline, they can be [customized](https://spacy.io/usage/processing-pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger <spacy.pipeline.Tagger object at 0x0000026695D05668>\n",
      "parser <spacy.pipeline.DependencyParser object at 0x0000026695DDFB48>\n",
      "ner <spacy.pipeline.EntityRecognizer object at 0x0000026695DDFBF8>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "for name,proc in nlp.pipeline:\n",
    "    print(name,proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.3 Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing is the process of splitting a text into individual components - words - for further processing. Although this might sound simple, the pecularities of the English language and how it is used often make tokenizing more complex than we might expect.\n",
    "\n",
    "To see some of the challenges, we will grab a specifc pre-chosen tweet and process it. For demonstration purposes, we will just use the text of the tweet.  \n",
    "\n",
    "This will give us a beginning feel for what [Spacy](https://spacy.io) can do, how we might use it, and how we might want to extend and revise the tokenizing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample='#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets have usage patterns that are non-standard English - URLs, hashtags, user references (this particularly tweet was not selected accidentally). These patterns create challenges for extracting content - we might want to know that \"#QuitSmoking\" is, in a tweet, a hashtag that should be considered as a complete unit.  \n",
    "\n",
    "We'll see soon how we might do this, but first, to start the NLP process, we can import the spaCy components and create an NLP object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then parse out the text from the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = nlp(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list of tokens. We can print out each token to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Smoking', 'affects', 'multiple', 'parts', 'of', 'our', 'body', '.', 'Know', 'more', ':', 'https://t.co/hwTeRdC9Hf', '\\n', '#', 'SwasthaBharat', '#', 'NHPIndia', '#', 'mCessation', '#', 'QuitSmoking', 'https://t.co/x7xHO9G2Cr']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see right away that this parsing isn't quite what we would like. Default English parsing treats  `#Smoking`  as two separate tokens - `#` and `Smoking`. Similar problems happen for other hashtags.\n",
    "\n",
    "To treat this as a hashtag, we will indeed need to revise the tokenizer. \n",
    "\n",
    "For another example of potential problems, consider this tweet text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '-', 'cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "smoketweet='E-cigarette use by teens linked to later tobacco smoking, study says https://t.co/AhTpFUw0TW'\n",
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"E-cigarette\" becomes three tokens. This is not what we want - we want it to be held together as one. \n",
    "\n",
    "We will revise the spaCy tokenizer to handle these two difficulties - hashtags and \"E-cigarette\" tokenizing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3.1 Exception rules\n",
    "\n",
    "\"E-cigarette\" can be handled with some simple exception rules.\n",
    "\n",
    "To do this, we can refer to the spaCy docuentation, which describes the process for adding a [special-case tokenizer rule](https://spacy.io/usage/linguistic-features#section-tokenization). Essentially, these rules allow for the possibility of adding new rules to customize parsing for specific domains:\n",
    "\n",
    "Each new rule will be a dictionary with three fields:\n",
    "    * `ORTH` is the text that will be matched\n",
    "    * `LEMMA` is the lemma form\n",
    "    * `POS` is the part-of-speech\n",
    "    \n",
    "These can then be added to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "nlp.tokenizer.add_special_case(u'E-cigarette', special_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands suggest the text \"e-cigarette\" should be handled by the special case rule saying that it is a single token. Now, let's take a look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e-cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we capture \"E-cigarette\" as one token. Note the importance of including both capitalizations in revised rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.3.2 Tokenizing hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicators of the progress and content of Twitter conversations, hashtags are important in tweets. For example, some analyses might want to use trends in hashtags, and their mentions in tweets and retweets, to understand conversational dynamics and the spread of ideas. However, as we saw, they are not handled properly by the deafult tokenier. As a reminder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Smoking', 'affects', 'multiple', 'parts', 'of', 'our', 'body', '.', 'Know', 'more', ':', 'https://t.co/hwTeRdC9Hf', '\\n', '#', 'SwasthaBharat', '#', 'NHPIndia', '#', 'mCessation', '#', 'QuitSmoking', 'https://t.co/x7xHO9G2Cr']\n"
     ]
    }
   ],
   "source": [
    "parsed = nlp(sample)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can look specifically at \"#Smoking\", which becomes two tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "Smoking\n"
     ]
    }
   ],
   "source": [
    "print(parsed[0])\n",
    "print(parsed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy.tokens.doc.Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how \"#Smoking\" is split into \"#\" and \"Smoking\". To avoid this, we will can add a specialized processing component as a member of a [spaCy pipeline](https://spacy.io/usage/processing-pipelines).\n",
    "\n",
    "To process hashtags, we will use code suggested by a [spaCy\n",
    "GitHub issue](https://github.com/explosion/spaCy/issues/503). To see how this should work, let's walkt through some steps:\n",
    "\n",
    "First, let's look at the tokens in the tweet parsed above. We can iterate through with enumerate. We can also look at a few interesting elements:\n",
    "\n",
    "* `nbor` gets the next token after a token.\n",
    "* `idx ` is the position of the token in the list of characters, starting at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 #\n",
      "1 Smoking\n",
      "9 affects\n",
      "17 multiple\n"
     ]
    }
   ],
   "source": [
    "print(str(parsed[0].idx)+\" \"+parsed[0].text)\n",
    "print(str(parsed[0].nbor().idx)+\" \"+str(parsed[0].nbor().text))\n",
    "print(str(parsed[1].nbor().idx)+\" \"+str(parsed[1].nbor().text))\n",
    "print(str(parsed[2].nbor().idx)+\" \"+str(parsed[2].nbor().text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, '#' starts of the string,  and 'Smoking' occupies characters 7 characters starting with character 1.  The 9th characer (index 8) is a space, so the next token ('affects') starts on the 10th character, which has index 9, etc.\n",
    "\n",
    "We can use this information to find a hash tag. essentially, we can look for a tag that has the text '#'. If we find one, we can look at the next tag and merge all of the characters from the start of the first tag to the end of the second tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "#Smoking"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=parsed[0].idx\n",
    "length = len(parsed[1].text)\n",
    "end = start+length+1\n",
    "print(str(start))\n",
    "print(str(end))\n",
    "parsed.merge(start,end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combines the character starting with 0 up until the character before the character at index 8 (which is a space) to form a new token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the list of tokens, we see that the first two are merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#Smoking', 'affects', 'multiple', 'parts', 'of', 'our', 'body', '.', 'Know', 'more', ':', 'https://t.co/hwTeRdC9Hf', '\\n', '#', 'SwasthaBharat', '#', 'NHPIndia', '#', 'mCessation', '#', 'QuitSmoking', 'https://t.co/x7xHO9G2Cr']\n"
     ]
    }
   ],
   "source": [
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get this to work for all of the tokens in a tweet, we need a routine that will repeatedly iterate over the tokens until we can't find anymore hashtags:\n",
    "\n",
    "[new hashtag_pipe(doc) function that can handle multiple \"#\" are written under bonus challenge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = True\n",
    "    while merged_hashtag == True:\n",
    "        merged_hashtag = False\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                try:\n",
    "                    nbor = token.nbor()\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text)+1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine might require a bit of explanation. The main routine in lines 6-16 does the bulk of the work shown above - we find a token that contains only the single character '#', we find the end of the next token, and we merge the two.\n",
    "\n",
    "There is one catch in that inner loop. If the last token in the string is a '#', the attempt to read the next token (on line 9) will cause an exception. If this happens, we're done anyway. So we `try` to get the next token. If it fails, we must be at the end of the document, so the `except`  clause does nothing, as indicated by the `pass`.\n",
    "\n",
    "However, this is not the whole story. The merging of these two tokens removes one from the list of tokens returned by `enumerate(doc)`. If we continue on, the result of the enumeration will evenutally blow  up, as the code will try to access an element in the set of tokens that is no longer there (try it and see). \n",
    "\n",
    "To get around this, we change the inner loop to `break` out as soon as a pair of tokens are merged. This will start the process over with a new enumeration. This process will repeat until we make it all the way through lines 6-16 - in other words, all of the way through the tweet -  without finding a pair of tokens to merge. When this happens, `merged_hashtag` will stay False, and the outer loop will exit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this routine written, we can then add it to the first position in the pipeline, which will put it after the default tokenizer, but before the part of speech tagger and other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(hashtag_pipe,first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter\n",
      "#hashtag\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"twitter #hashtag\")\n",
    "print(doc[0].text)\n",
    "print(doc[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our first example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \n",
      "#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr\n",
      "\n",
      "['#Smoking', 'affects', 'multiple', 'parts', 'of', 'our', 'body', '.', 'Know', 'more', ':', 'https://t.co/hwTeRdC9Hf', '\\n', '#SwasthaBharat', '#NHPIndia', '#mCessation', '#QuitSmoking', 'https://t.co/x7xHO9G2Cr']\n"
     ]
    }
   ],
   "source": [
    "sample='#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'\n",
    "print(sample+\"\\n\")\n",
    "parsed = nlp(sample)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try a tweet that ends with a '#':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twitter', '#hashtag', '#']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"twitter #hashtag #\")\n",
    "print([tok.text for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can also try a pathological example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weird', 'hashtag', '##', '#tag']\n"
     ]
    }
   ],
   "source": [
    "parsed = nlp(\"weird hashtag ###tag\")\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops. That doesn't work. It's not even clear that this is a legal hashtag. \n",
    "\n",
    "**BONUS CHALLENGE**: Perhaps you can extend the routine to make it handle hashtags started by multiple '#' symbols?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = True\n",
    "    while merged_hashtag == True:\n",
    "        merged_hashtag = False\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                try:\n",
    "                    nbor = token.nbor()\n",
    "                    while nbor.text=='#':\n",
    "                        nbor = nbor.nbor()\n",
    "                        token=token.nbor()\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text)+1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "    return doc\n",
    "\n",
    "nlp.remove_pipe('hashtag_pipe')\n",
    "nlp.add_pipe(hashtag_pipe,first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weird', '###hashtag', '###tag', 'new', '#one', '####four']\n"
     ]
    }
   ],
   "source": [
    "parsed = nlp(\"weird ###hashtag ###tag new #one ####four\")\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the \"#\" before a word are merged with words into one hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing, we can combine the changes to the tokenizer, wrapping them up in a subroutine as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "\n",
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        while nbor.text=='#':\n",
    "                            nbor = nbor.nbor()\n",
    "                            token=token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = getTwitterNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weird', 'e-cigarette', 'hashtag', '###tag']\n"
     ]
    }
   ],
   "source": [
    "parsed = nlp(\"weird e-cigarette hashtag ###tag\")\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that spaCy can also detect sentences. If you have multiple sentences, they will be found in the results of the parser as spans, each with a start and endpoint, given in terms of the positions of the tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed= nlp(\"This is an example of parsing two sentences. Here is the second sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9\n",
      "9 15\n"
     ]
    }
   ],
   "source": [
    "for span in parsed.sents:\n",
    "    print(str(span.start)+\" \"+str(span.end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the first sentence includes token 0-8 and the second includes 9-14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to access the text of the sentences directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example of parsing two sentences.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(parsed.sents)\n",
    "sents[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      ".\n",
      "Here\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(parsed[0].text)\n",
    "print(parsed[8].text)\n",
    "print(parsed[9].text)\n",
    "print(parsed[14].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers are traditionally built using optimized [regular expressions](https://www.regular-expressions.info/). For more information about tokenizing in paCy, see [spaCy 101](https://spacy.io/usage/spacy-101#section-features) and the [detailed discussion of the spaCy tokenizer](https://spacy.io/usage/linguistic-features#tokenization). For a more general introduction, see [Chapter 2 of Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3.3 Lemmatization, stop words, and alpha characterization\n",
    "\n",
    "The spaCy tokenizer proivdes a few other useful features along the way:\n",
    "\n",
    "* Lemmatization: For each token, spaCy can find the *lemma*: the \"standard\" or \"base\" form, reducing verb forms to their base verb, plurals to appropriate singular nouns, etc.  \n",
    "* Stop word identification - labelling words as commonly-found words taht add little or no information.\n",
    "* Alphanumeric identification - identifying those tokens that contain only alphanumeric values.\n",
    "\n",
    "To see these in action, let's review a few tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \n",
      "#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr\n",
      "affects\n",
      "17543419487618836897\n",
      "affect\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "sample='#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "print(parsed[1].text)\n",
    "print(parsed[1].lemma)\n",
    "print(parsed[1].lemma_)\n",
    "print(parsed[1].is_stop)\n",
    "print(parsed[1].is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `affects` has the lemma `affect`.  Note that spaCy stores many fields as both hashes for efficiency and as text  for readability. You'll want to use the text form for interpreting results, but the hash for computing. They differ only in the use of the trailing underscore - thus `lemma` is the hash while `lemma_` is the human readable form.\n",
    "\n",
    "We can also see that `affect` is not a stop word, and it is alphabetic.\n",
    "\n",
    "Some NLP systems will go a bit further than spaCy's lemmatization, using a process called \"stemming\" to reduce words to base forms. With a stemming algorithm, \"scared\" might be reduced to \"scare\" - see this description of [Porter's stemming algorithm](https://tartarus.org/martin/PorterStemmer/) for more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play around a bit, you might notice that even very common words don't get called stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, False\n",
      "is, True\n",
      "a, True\n",
      "test, False\n",
      "of, True\n",
      "the, True\n",
      "stop, False\n",
      "word, False\n",
      "tool, False\n"
     ]
    }
   ],
   "source": [
    "text=\"This is a test of the stop word tool\"\n",
    "doc=nlp(text)\n",
    "for d in doc:\n",
    "    print(d.text+\", \"+str(d.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, 'This', 'a',' of', and 'the' should be considered stop words. This is a bit of a minor bug. This [Stack Overflow](https://stackoverflow.com/questions/41170726/add-remove-stop-words-with-spacy) post provides a workaround:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, False\n",
      "is, True\n",
      "a, True\n",
      "test, False\n",
      "of, True\n",
      "the, True\n",
      "stop, False\n",
      "word, False\n",
      "tool, False\n"
     ]
    }
   ],
   "source": [
    "text=\"This is a test of the stop word tool\"\n",
    "doc=nlp(text)\n",
    "for d in doc:\n",
    "    print(d.text+\", \"+str(d.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better. Tying this together with the hashtag pipe routine above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, False\n",
      "is, True\n",
      "a, True\n",
      "test, False\n",
      "of, True\n",
      "the, True\n",
      "stop, False\n",
      "word, False\n",
      "tool, False\n"
     ]
    }
   ],
   "source": [
    "text=\"This is a test of the stop word tool\"\n",
    "nlp=getTwitterNLP()\n",
    "doc=nlp(text)\n",
    "for d in doc:\n",
    "    print(d.text+\", \"+str(d.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.4  Part-Of-Speech Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in NLP is *Part of speech tagging* - classifying each token as one of the parts of speech that we all learned in elementrary school. Parts of speech are assigned to attributes of each token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affects\n",
      "99\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "sample='#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'\n",
    "parsed=nlp(sample)\n",
    "print(parsed[1].text)\n",
    "print(parsed[1].pos)\n",
    "print(parsed[1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before, we have two attributes here - `pos` is the hash code for the part of speech, used for efficiency, while `pos_` is the human readable form. Other attributes derived by spaCy follow the same pattern.\n",
    "\n",
    "A second attribute - `tag` - provide es additional information.\n",
    "\n",
    "As described in the [spaCy documentation for part-of-speech tags](https://spacy.io/api/annotation#pos-tagging), the tags associated with these two fields come from different sources. 'tag_' uses parts-of-speech from a version of the [Penn Treebank](https://www.seas.upenn.edu/~pdtb/), a well-known corpus of annotated text. 'pos_' uses a simpler set of tags from [A Universal Part-of-Speech Tagset](https://arxiv.org/abs/1104.2086), published by researchers from Google.  \n",
    "\n",
    "The tags for `affects` provide an example of the difference. According to the [spaCy documentation ](https://spacy.io/api/annotation#pos-tagging) `VBZ` from the Penn tag set indicates a 'verb, 3rd person singular present', while 'the 'VERB' result for 'pos_' is a more general tag from the Google set. There are many types of verbs in the Penn Treebank that correspond tot the 'VERB' tag from the Google set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affects\n",
      "VBZ\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(parsed[1].text)\n",
    "print(parsed[1].tag_)\n",
    "print(parsed[1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about a part of spech tag, you can use `spacy.explain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb\n",
      "verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(parsed[1].pos_))\n",
    "print(spacy.explain(parsed[1].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at token 0 (\"#Smoking\"), token 3 (\"parts\"), token 11 (\"https://t.co/hwTeRdC9Hf'\"),  and token 13(\"#SwasthaBharat\") to see a few more tokens in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Smoking #smok VERB VBG False False\n",
      "parts part NOUN NNS False True\n",
      "https://t.co/hwTeRdC9Hf https://t.co/hwterdc9hf PROPN NNP False False\n",
      "#SwasthaBharat #swasthabharat NOUN NN False False\n"
     ]
    }
   ],
   "source": [
    "t0 = parsed[0]\n",
    "t3 = parsed[3]\n",
    "t11= parsed[11]\n",
    "t13 = parsed[13]\n",
    "print (t0.text,t0.lemma_,t0.pos_,t0.tag_,t0.is_stop,t0.is_alpha)\n",
    "print (t3.text,t3.lemma_,t3.pos_,t3.tag_,t3.is_stop,t3.is_alpha)\n",
    "print (t11.text,t11.lemma_,t11.pos_,t11.tag_,t11.is_stop,t11.is_alpha)\n",
    "print (t13.text,t13.lemma_,t13.pos_,t13.tag_,t13.is_stop,t13.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that URLS are neither alphabetical  nor stop-words, but they are proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's turn the code that we used above into a routine, along with a routine to print out token details and try another tweet or two. To make things easy to read, we'll use some spaces to format things in columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTokDetails(parsed):\n",
    "    print(\"{:25} {:25} {:7}{:7}{:7}{:7}\".format(\"Token text\",\"Lemma\",\"POS\",\"Tag\",\"Stop?\",\"Alpha?\"))\n",
    "    for tok in parsed:\n",
    "        print(\"{:25} {:25} {:7}{:7}{:7}{:7}\".format(str(tok.text),str(tok.lemma_),str(tok.pos_),str(tok.tag_),str(tok.is_stop),str(tok.is_alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "sample2 = smoking.getText(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rev, Sharpton lit the fuse in so many situations where mediation was needed stoking raw emotions for explosive outcomes. He was holed up in a luxury hotel a few miles from Ferguson smoking an expensive cigar and drinking upper shelf liquor while all hell was breaking loose there. https://t.co/ltGefEiv8V'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed2=nlp(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "Rev                       rev                       PROPN  NNP    False  True   \n",
      ",                         ,                         PUNCT  ,      False  False  \n",
      "Sharpton                  sharpton                  PROPN  NNP    False  True   \n",
      "lit                       light                     VERB   VBD    False  True   \n",
      "the                       the                       DET    DT     True   True   \n",
      "fuse                      fuse                      NOUN   NN     False  True   \n",
      "in                        in                        ADP    IN     True   True   \n",
      "so                        so                        ADV    RB     True   True   \n",
      "many                      many                      ADJ    JJ     True   True   \n",
      "situations                situation                 NOUN   NNS    False  True   \n",
      "where                     where                     ADV    WRB    True   True   \n",
      "mediation                 mediation                 NOUN   NN     False  True   \n",
      "was                       be                        VERB   VBD    True   True   \n",
      "needed                    need                      VERB   VBN    False  True   \n",
      "stoking                   stoke                     VERB   VBG    False  True   \n",
      "raw                       raw                       ADJ    JJ     False  True   \n",
      "emotions                  emotion                   NOUN   NNS    False  True   \n",
      "for                       for                       ADP    IN     True   True   \n",
      "explosive                 explosive                 ADJ    JJ     False  True   \n",
      "outcomes                  outcome                   NOUN   NNS    False  True   \n",
      ".                         .                         PUNCT  .      False  False  \n",
      "He                        -PRON-                    PRON   PRP    False  True   \n",
      "was                       be                        VERB   VBD    True   True   \n",
      "holed                     hole                      VERB   VBN    False  True   \n",
      "up                        up                        PART   RP     True   True   \n",
      "in                        in                        ADP    IN     True   True   \n",
      "a                         a                         DET    DT     True   True   \n",
      "luxury                    luxury                    NOUN   NN     False  True   \n",
      "hotel                     hotel                     NOUN   NN     False  True   \n",
      "a                         a                         DET    DT     True   True   \n",
      "few                       few                       ADJ    JJ     True   True   \n",
      "miles                     mile                      NOUN   NNS    False  True   \n",
      "from                      from                      ADP    IN     True   True   \n",
      "Ferguson                  ferguson                  PROPN  NNP    False  True   \n",
      "smoking                   smoke                     VERB   VBG    False  True   \n",
      "an                        an                        DET    DT     True   True   \n",
      "expensive                 expensive                 ADJ    JJ     False  True   \n",
      "cigar                     cigar                     NOUN   NN     False  True   \n",
      "and                       and                       CCONJ  CC     True   True   \n",
      "drinking                  drink                     VERB   VBG    False  True   \n",
      "upper                     upper                     ADJ    JJ     False  True   \n",
      "shelf                     shelf                     NOUN   NN     False  True   \n",
      "liquor                    liquor                    NOUN   NN     False  True   \n",
      "while                     while                     ADP    IN     True   True   \n",
      "all                       all                       DET    DT     True   True   \n",
      "hell                      hell                      NOUN   NN     False  True   \n",
      "was                       be                        VERB   VBD    True   True   \n",
      "breaking                  break                     VERB   VBG    False  True   \n",
      "loose                     loose                     ADV    RB     False  True   \n",
      "there                     there                     ADV    RB     True   True   \n",
      ".                         .                         PUNCT  .      False  False  \n",
      "https://t.co/ltGefEiv8V   https://t.co/ltgefeiv8v   X      ADD    False  False  \n"
     ]
    }
   ],
   "source": [
    "printTokDetails(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see some interesting pattners arising here.  For example:\n",
    "\n",
    "* We see many different type of speech. Initially, we might want to focus on the nouns alone, as they provide much of the content.  \n",
    "\n",
    "* Look for words like \"is\" or \"was\" - these might all refer to a common lemma term - \"be\", corresponding to the generic form of he verb. Do you see any other incidents of lemma forms that differ from the parsed text?\n",
    "\n",
    "* URLs and icons might be present in tweets. Are they classified as alphanumeric? Should we include them as part of the \"useful\" text from a tweet? \n",
    "\n",
    "How about another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Tombstoneaz #Tombstone #Az #Arizona #DOCSBARAZ #Carrafhole #docsbaraz #DOCSBAR #docsbar #TOMBSTONE in the #bathroom or #smoking #patio https://t.co/2EAwQDsJEB'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "sample2 = smoking.getText(tweet_id)\n",
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "#Tombstoneaz              #tombstoneaz              VERB   VBD    False  False  \n",
      "#Tombstone                #tombstone                ADJ    JJ     False  False  \n",
      "#Az                       #az                       VERB   VBG    False  False  \n",
      "#Arizona                  #arizona                  ADP    IN     False  False  \n",
      "#DOCSBARAZ                #docsbaraz                NOUN   NN     False  False  \n",
      "#Carrafhole               #carrafhole               NOUN   NN     False  False  \n",
      "#docsbaraz                #docsbaraz                NOUN   NN     False  False  \n",
      "#DOCSBAR                  #docsbar                  NOUN   NN     False  False  \n",
      "#docsbar                  #docsbar                  NOUN   NN     False  False  \n",
      "#TOMBSTONE                #tombstone                VERB   VBG    False  False  \n",
      "in                        in                        ADP    IN     True   True   \n",
      "the                       the                       DET    DT     True   True   \n",
      "#bathroom                 #bathroom                 NOUN   NN     False  False  \n",
      "or                        or                        CCONJ  CC     True   True   \n",
      "#smoking                  #smok                     VERB   VBG    False  False  \n",
      "#patio                    #patio                    NOUN   NNS    False  False  \n",
      "https://t.co/2EAwQDsJEB   https://t.co/2eawqdsjeb   NOUN   NNS    False  False  \n"
     ]
    }
   ],
   "source": [
    "parsed2=nlp(sample2)\n",
    "printTokDetails(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a few more of these to get a bit more of a feel for he distribution of lemmas and POS tags. The following shortcut routine will make this a bit easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomTweetText(tweets):\n",
    "    tweet_id = random.choice(list(tweets.getIds()))\n",
    "    return tweets.getText(tweet_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXERCISE 3.1: Filtering tokens\n",
    "\n",
    "Although NLP parsing is often a good start, further filtering is often necessary to focus on data relevant for specific tasks. In this problem, we will review some additional tweets and develop a post-processing routine capable of filtering tweets as necessary for our needs. \n",
    "\n",
    "3.1.1 Using the `getRandomTweetText`, and `printTokDetails` routines above, aong with the spaCy `parser` command, examine several tweets to decide which tokens should be included or not.  List criteria for keeeping/removing tokens. Remember to use `spacy.explain()` for any unfamiliar POS or tag entries. Note that your  criteria will not be perfect, and will likely need refinining. Examine enough tweets to feel confident in your criteria. Because we are parsing tweets, please don't forget hashtags and user mentions.\n",
    "\n",
    "3.1.2 Write a routine  `includeToken` that will return a token to be inclued if it matches the criteria that you identified in 3.11, and false otherwise.  Assume for now that we are only interested in nouns and verbs, as they might be a good starting point to find information about vaping or smoking. For any tokens that are included,`includeToken` should return the lemmatized-version of the token, converted to all lower-case and stripped of any whitespace, using `strip()`. Zero-length tokens should not be included.\n",
    "\n",
    "3.1.3 Write a routine `filterTweetTokens` that will filter the parsed tokens from a single tweet, returning a list of the tokens to be included, based on your criteria from `includeToken`. To standardize matters, `filterTweetTokens` should also return the lemmatized-version of the token, converted to all lower-case.\n",
    "\n",
    "3.1.4 Run `filterTweetTokens` on a few tweets. Identify any inaccuracies and explain them. When possible, identify an approach for improving performance, and implement it in a revision version of `filterTweetTokens`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS insert here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets5=Tweets(\"UPMC\",49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Through the strategic expansion of #telehealth capabilities, UPMC has been at the fore-front of leveraging technology to give patients expanded options to access world-class clinical care: https://t.co/oCTAKhnYnG https://t.co/KLKrEjkb0v\n",
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "Through                   through                   ADP    IN     False  True   \n",
      "the                       the                       DET    DT     True   True   \n",
      "strategic                 strategic                 ADJ    JJ     False  True   \n",
      "expansion                 expansion                 NOUN   NN     False  True   \n",
      "of                        of                        ADP    IN     True   True   \n",
      "#telehealth               #telehealth               NOUN   NN     False  False  \n",
      "capabilities              capability                NOUN   NNS    False  True   \n",
      ",                         ,                         PUNCT  ,      False  False  \n",
      "UPMC                      upmc                      PROPN  NNP    False  True   \n",
      "has                       have                      VERB   VBZ    True   True   \n",
      "been                      be                        VERB   VBN    True   True   \n",
      "at                        at                        ADP    IN     True   True   \n",
      "the                       the                       DET    DT     True   True   \n",
      "fore                      fore                      ADJ    JJ     False  True   \n",
      "-                         -                         PUNCT  HYPH   False  False  \n",
      "front                     front                     NOUN   NN     True   True   \n",
      "of                        of                        ADP    IN     True   True   \n",
      "leveraging                leverage                  VERB   VBG    False  True   \n",
      "technology                technology                NOUN   NN     False  True   \n",
      "to                        to                        PART   TO     True   True   \n",
      "give                      give                      VERB   VB     True   True   \n",
      "patients                  patient                   NOUN   NNS    False  True   \n",
      "expanded                  expand                    VERB   VBN    False  True   \n",
      "options                   option                    NOUN   NNS    False  True   \n",
      "to                        to                        PART   TO     True   True   \n",
      "access                    access                    VERB   VB     False  True   \n",
      "world                     world                     NOUN   NN     False  True   \n",
      "-                         -                         PUNCT  HYPH   False  False  \n",
      "class                     class                     NOUN   NN     False  True   \n",
      "clinical                  clinical                  ADJ    JJ     False  True   \n",
      "care                      care                      NOUN   NN     False  True   \n",
      ":                         :                         PUNCT  :      False  False  \n",
      "https://t.co/oCTAKhnYnG   https://t.co/octakhnyng   NOUN   NN     False  False  \n",
      "https://t.co/KLKrEjkb0v   https://t.co/klkrejkb0v   NOUN   NNS    False  False  \n"
     ]
    }
   ],
   "source": [
    "text5=getRandomTweetText(tweets5);\n",
    "parsed5=nlp(text5)\n",
    "print(text5)\n",
    "printTokDetails(parsed5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria:\n",
    "1. Remove tokens containing only stop words;\n",
    "2. remove tokens whose pos is ADP,CCONJ or DET (some CCONJ like & are not considered as stop words);\n",
    "3. remove tokens with pos 'PUNCT' and length lower than 3 (some short hashtags will be recognized as 'PUNCT', such as \"#EHR\", so we should not remove all 'PUNCT' tokens);\n",
    "4. remove tokens with pos 'PROPN' which also include symbols;\n",
    "5. remove tokens with Lemma -PRON-."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Criteria identified in 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['strategic',\n",
       " 'expansion',\n",
       " '#telehealth',\n",
       " 'capability',\n",
       " 'upmc',\n",
       " 'fore',\n",
       " 'leverage',\n",
       " 'technology',\n",
       " 'patient',\n",
       " 'expand',\n",
       " 'option',\n",
       " 'access',\n",
       " 'world',\n",
       " 'class',\n",
       " 'clinical',\n",
       " 'care',\n",
       " 'https://t.co/octakhnyng',\n",
       " 'https://t.co/klkrejkb0v']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def includeToken(parsed):\n",
    "    tokens=[]\n",
    "    for tok in parsed:\n",
    "        if tok.is_stop==False and tok.pos_!='CCONJ' and tok.pos_!='ADP' and tok.pos_!='DET' and tok.lemma_!='-PRON-':\n",
    "            if tok.pos_=='PUNCT' or (tok.pos_=='PROPN' and tok.is_alpha==False):\n",
    "                if len(tok.lemma_.strip())>4 or (tok.pos_!='PUNCT' and len(tok.lemma_.strip())>3):\n",
    "                    tokens.append(tok.lemma_.strip())\n",
    "            else:\n",
    "                tokens.append(tok.lemma_.strip())\n",
    "    return tokens;\n",
    "\n",
    "tokens=includeToken(parsed5);\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) \"Assume for now\" rules in 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expansion',\n",
       " '#telehealth',\n",
       " 'capability',\n",
       " 'upmc',\n",
       " 'leverage',\n",
       " 'technology',\n",
       " 'patient',\n",
       " 'expand',\n",
       " 'option',\n",
       " 'access',\n",
       " 'world',\n",
       " 'class',\n",
       " 'care',\n",
       " 'https://t.co/octakhnyng',\n",
       " 'https://t.co/klkrejkb0v']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def includeToken2(parsed):\n",
    "    tokens=[]\n",
    "    for tok in parsed:\n",
    "        if tok.is_stop==False and (tok.pos_=='NOUN' or tok.pos_=='VERB' or tok.pos_=='PROPN'):\n",
    "            if len(tok.text)>2:\n",
    "                tokens.append(tok.lemma_.strip())\n",
    "    return tokens;\n",
    "\n",
    "tokens=includeToken2(parsed5);\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetTokens(tweets):\n",
    "    text=getRandomTweetText(tweets);\n",
    "    parsed=nlp(text)\n",
    "    return includeToken(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'cnn',\n",
       " 'will',\n",
       " 'not',\n",
       " 'show',\n",
       " 'trump',\n",
       " 'upmc',\n",
       " 'presbyterian',\n",
       " 'hospital',\n",
       " '#maga',\n",
       " 'https://t.co/mroox8qzw5',\n",
       " '@youtube',\n",
       " '',\n",
       " 'will',\n",
       " 'not',\n",
       " '#fakenewsmedia',\n",
       " '#voteredstraightticket']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterTweetTokens(tweets5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detox',\n",
       " 'cleans',\n",
       " 'new',\n",
       " 'craze',\n",
       " 'work',\n",
       " 'https://t.co/lv0oayhwae',\n",
       " 'https://t.co/clmczslwqt']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterTweetTokens(tweets5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['klaus',\n",
       " 'bielefeldt',\n",
       " 'md',\n",
       " 'phd',\n",
       " 'director',\n",
       " 'neurogastroenterology',\n",
       " 'amp',\n",
       " 'motility',\n",
       " 'center',\n",
       " 'upmc',\n",
       " 'discuss',\n",
       " 'active',\n",
       " 'listening',\n",
       " 'conversation',\n",
       " 'framing',\n",
       " 'improved',\n",
       " 'education',\n",
       " 'significantly',\n",
       " 'improve',\n",
       " 'patient',\n",
       " 's',\n",
       " 'ability',\n",
       " 'manage',\n",
       " 'chronic',\n",
       " 'condition',\n",
       " 'https://t.co/y8to7q0mej']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterTweetTokens(tweets5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inaccuracies:\n",
    "Empty token is found in the second test case.\n",
    "\n",
    "Revision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetTokens(tweets):\n",
    "    tokens=[]\n",
    "    text=getRandomTweetText(tweets);\n",
    "    parsed=nlp(text)\n",
    "    tokens=includeToken(parsed)\n",
    "    while tokens.count('')!=0:\n",
    "            tokens.remove('')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'cnn',\n",
       " 'will',\n",
       " 'not',\n",
       " 'show',\n",
       " 'trump',\n",
       " 'upmc',\n",
       " 'presbyterian',\n",
       " 'hospital',\n",
       " '#maga',\n",
       " 'https://t.co/mroox8qzw5',\n",
       " '@youtube',\n",
       " 'will',\n",
       " 'not',\n",
       " '#fakenewsmedia',\n",
       " '#voteredstraightticket']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterTweetTokens(tweets5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that the empty token '' after token '@youtube' is removed (compared to Out [78])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END OF ANSWER*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will come back to these routines in [Part 4](SocialMedia%20-%20Part%203.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.6  Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dependency parsing* is the process of identifying the syntactic linkages between elements in a sentence. Dependency parsers lin noun phrases and modifiers, subjects to objects, etc. The [spaCy description of dependency parsing](https://spacy.io/usage/linguistic-features#dependency-parse) provides a detailed introduction - here, we provide a brief summary.\n",
    "\n",
    "To see the dependencies in action, we can iterate through the tokens, printing out the dependencies, and the head (ie, the token that a token depens upon, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \n",
      "#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr\n",
      "----\n",
      "multiple parts parts dobj affects\n",
      "our body body pobj of\n",
      "https://t.co/hwTeRdC9Hf \n",
      "#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr https://t.co/x7xHO9G2Cr ROOT https://t.co/x7xHO9G2Cr\n"
     ]
    }
   ],
   "source": [
    "sample='#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'\n",
    "print(sample)\n",
    "print(\"----\")\n",
    "parsed=nlp(sample)\n",
    "for chunk in parsed.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Smoking nsubj affects VERB\n",
      "affects ROOT affects VERB\n",
      "multiple amod parts ADJ\n",
      "parts dobj affects NOUN\n",
      "of prep parts ADP\n",
      "our poss body ADJ\n",
      "body pobj of NOUN\n",
      ". punct affects PUNCT\n",
      "Know ROOT Know VERB\n",
      "more advmod Know ADV\n",
      ": punct Know PUNCT\n",
      "https://t.co/hwTeRdC9Hf amod #NHPIndia PROPN\n",
      "\n",
      "  https://t.co/hwTeRdC9Hf SPACE\n",
      "#SwasthaBharat compound #NHPIndia NOUN\n",
      "#NHPIndia nmod https://t.co/x7xHO9G2Cr PROPN\n",
      "#mCessation compound #QuitSmoking NOUN\n",
      "#QuitSmoking amod https://t.co/x7xHO9G2Cr NOUN\n",
      "https://t.co/x7xHO9G2Cr ROOT https://t.co/x7xHO9G2Cr NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in parsed:\n",
    "    print(token.text,token.dep_,token.head.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a few things from this example:\n",
    "\n",
    "1. `#Smoking` is a noun subject of the sentence, dependent on the verb `affects`.\n",
    "2. `affects` is a verb at the ROOT level.\n",
    "3. `multiple` is an adjective modifier that modifies `parts`.\n",
    "4. `parts` is a noun that is the direct object of `affects`, etc..\n",
    "\n",
    "We can look in more deail a the text, dependency,  head, and children, of each token.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printParseTree(parsed):\n",
    "    print(\"{:10} {:10} {:7} {:7} {:30}\".format(\"Token text\",\"dep\",\"Head text\",\"POS\",\"Children\"))\n",
    "    for tok in parsed:\n",
    "        children=[child.text for child in tok.children]\n",
    "        children=\",\".join(children)\n",
    "        print(\"{:10} {:10} {:7} {:7} {:30}\".format(str(tok.text),str(tok.dep_),str(tok.head.text),str(tok.head.pos_),children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Smoking affects multiple parts of our body.\n",
      "Token text dep        Head text POS     Children                      \n",
      "#Smoking   nsubj      affects VERB                                  \n",
      "affects    ROOT       affects VERB    #Smoking,parts,.              \n",
      "multiple   amod       parts   NOUN                                  \n",
      "parts      dobj       affects VERB    multiple,of                   \n",
      "of         prep       parts   NOUN    body                          \n",
      "our        poss       body    NOUN                                  \n",
      "body       pobj       of      ADP     our                           \n",
      ".          punct      affects VERB                                  \n"
     ]
    }
   ],
   "source": [
    "sents =list(parsed.sents)\n",
    "print(sents[0])\n",
    "printParseTree(sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that that 'affects' is the root verb, with `#Smoking` as a noun subjects and `parts` as the object. `Parts`  is modified by `muliple` and `of our body`'.  \n",
    "\n",
    "We can use the [displacy](https://spacy.io/usage/visualizers#section-dep) renderer to show a graphical depiction of the dependencies. Since displacy seems to prefer showing thepare tree fror an entire document, we'll try it on a single sentence.\n",
    "\n",
    "Note - the \"%%capture\" line below tells Jupyter to hide some very ugly errors, whie still displaying the nice graphical result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"680\" height=\"227.0\" style=\"max-width: none; height: 227.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">#Smoking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">affects</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">multiple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">parts</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">our</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">body.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,92.0 C70,47.0 135.0,47.0 135.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,94.0 L62,82.0 78,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M250,92.0 C250,47.0 315.0,47.0 315.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,94.0 L242,82.0 258,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M160,92.0 C160,2.0 320.0,2.0 320.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M320.0,94.0 L328.0,82.0 312.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M340,92.0 C340,47.0 405.0,47.0 405.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M405.0,94.0 L413.0,82.0 397.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M520,92.0 C520,47.0 585.0,47.0 585.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,94.0 L512,82.0 528,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M430,92.0 C430,2.0 590.0,2.0 590.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M590.0,94.0 L598.0,82.0 582.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "from spacy import displacy\n",
    "\n",
    "text=\"#Smoking affects multiple parts of our body.\"\n",
    "parsed=nlp(text)\n",
    "displacy.render(docs=[parsed],jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows the structure given above in the printed version of the parse tree. \n",
    "\n",
    "These relationships might be useful for some NLP goals, particularly those involving relationships between concpets. \n",
    "\n",
    "A variety of approaches - including greedy algorithms, graph-based methods, and machine learning - can be used to extract dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.7 Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Named entity recognition* is the process of extracting categories to known entities - places, people, things, ec. spaCy provides a statistical model capable of assigning an [entity type](https://spacy.io/api/annotation#named-entities) to many of the terms in a document. For an example, let's look at the entities found in a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Gottlieb of @FDA says 8 million lives could be saved by cutting nicotine levels #publichealth https://phony.url/123\n",
      "----\n",
      "Scott Gottlieb PERSON\n",
      "8 million CARDINAL\n"
     ]
    }
   ],
   "source": [
    "sample ='Scott Gottlieb of @FDA says 8 million lives could be saved by cutting nicotine levels #publichealth https://phony.url/123'\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "print(\"----\")\n",
    "for ent in parsed.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that entities are not equivalent to tokens: `Scott Gottlieb` and `8 million` are entities, but not tokens. For comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scott', 'Gottlieb', 'of', '@FDA', 'says', '8', 'million', 'lives', 'could', 'be', 'saved', 'by', 'cutting', 'nicotine', 'levels', '#publichealth', 'https://phony.url/123']\n"
     ]
    }
   ],
   "source": [
    "print([tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, two tokens - `Scott` and `Gottlieb` are combined to form a single entity - `Scott Gottlieb'.' We can modify the above to see where each entity starts and ends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text            Start End   Type \n",
      "Scott Gottlieb      0    14 PERSON\n",
      "8 million          28    37 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "print(\"{:15} {:5} {:5} {:5}\".format(\"Text\",\"Start\",\"End\",\"Type\"))\n",
    "for ent in parsed.ents:\n",
    "    print(\"{:15} {:5} {:5} {:5}\".format(ent.text,ent.start_char,ent.end_char,ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, `Scott Gottlieb` starts at character 0 and goes up through (but not including) character 14.\n",
    "\n",
    "We can also use the spaCy visualizer to look at the named entities in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Scott Gottlieb\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " of @FDA says \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    8 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " lives could be saved by cutting nicotine levels #publichealth https://phony.url/123</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "displacy.render(docs=[parsed],jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's try another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many people in New York are smokers?\n",
      "----\n",
      "Text            Start End   Type \n",
      "New York           19    27 GPE  \n"
     ]
    }
   ],
   "source": [
    "sample='How many people in New York are smokers?'\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "print(\"----\")\n",
    "print(\"{:15} {:5} {:5} {:5}\".format(\"Text\",\"Start\",\"End\",\"Type\"))\n",
    "for ent in parsed.ents:\n",
    "    print(\"{:15} {:5} {:5} {:5}\".format(ent.text,ent.start_char,ent.end_char,ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a named entity type, `GPE` stands for geopolitical entity.\n",
    "\n",
    "Here, we note that hashtags are not necessarily categorized as entities.  This might be a shortcoming if we were going to use named entities as part of our strategy for classiying tweets. The spaCy named entity recognizer is based on statistical models that can be extended given enough training data. See the discussion of [training the named entity recognizer](https://spacy.io/usage/training#section-ner) for details on how this might be done. \n",
    "\n",
    "*Challenge*: Collect some tweets with hashtags and train the spaCy named entity recognizer add a `HASHTAG` as a new entity type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2\n",
    "\n",
    "The natural language processing pipeline consists of several processes that add substantial structure to our understanding of these Tweets. Tokenizing, part of speech tagging, lemmatiziation, dependency parsing, and named entity recognition each add different details that might be used to understand and classify documents, while also providing some hints as to interesting questions that we might ask.\n",
    "\n",
    "Review some tweets and discuss any patterns or questions that arise. You might consider some of the following:\n",
    "\n",
    "* Are there terms that show up more frequently in the vaping tweets as opposed to the smoking tweets?\n",
    "* Are the tokens that we filtered (in Exercise 3.1) useful, or do we need the whole set of tokens to inerpret\n",
    "* Are the named entities informative?\n",
    "\n",
    "Describe any other interesting phenomena that you think you might see in the corpus. Note that this question is not asking for fully statistically supported models. Rather, we're just looking for things that might be interesting to pursue further: it may turn out that any \"patterns\" you identify here are just incidental.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS - insert answer here *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has been in existence for about 4 years, which is a relatively long time in the rapid evolving NLP field. From 3.1.6 we can observe that Spacy with the corpus does show some abilities of parsing sentence, but it seems to be quite weak for analyzing prepositional phrase instead of the main structure in a sentence. On the other hand, Spacy is also lack of the ability for analyzing the relationship cross different sentence. Some deep learning based NLP pre-trained models, such as the BERT model of google that is based on Tensorflow, perform a lot better in various complex dependency analysis task, no matter cross different sentences or inside one sentence. As one thing interesting to pursue further for the future version of this exercise/module, you may try to combine new powerful models like BERT to the present version, encouraging future students to give a try on the new models, and compare results of these models and the original Spacy one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END OF ANSWER*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Comparing Vocabularies\n",
    "\n",
    "Although the examination of a few selected tweets might help us understand some of the trends in terminology and how they differ between the `smoking` and the `vaping` sets, these spot checks may not give a balanced picture of text usage across both of the corpora.  Here we will try to more systematically address the questions that you considered in Exercise 3.2.\n",
    "\n",
    "\n",
    "A most systematic way to go about this would be to identify frequently-occurring tokens in  both corpora, using methods similar to those used in our examination of frequent authors from  [Part 1](SocialMedia%20-%20Part%201.ipynb) and in `getCodeProfile()` from [Part 2](SocialMedia%20-%20.ipynb). Specifically, we will write a routine that iterates through all of the tweets in a Tweets object and does the following:\n",
    "\n",
    "1. Parse the tweet\n",
    "2. Filter tokens (using the routines developed above).\n",
    "3. Adds each token to a hash assoicating each token with a count of the number of times it has appeared in the corpus\n",
    "\n",
    "The result will be a hash with the number of times each term occurs in the corpus. We can then sort this hash by descending values of the count to find the most frequent terms, and we can comapare results for the two sets. We'll return this information in two forms - a hash (for quick access) and a list (for sorting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequentTerms(tweets,filtered=True):\n",
    "    frequents={}\n",
    "    for id in tweets.getIds():\n",
    "        text = tweets.getText(id)\n",
    "        parsed=nlp(text)\n",
    "        if filtered ==True:\n",
    "            toks = filterTweetTokens(tweets)\n",
    "        else:\n",
    "            toks = [tok for tok in parsed]\n",
    "        \n",
    "        for tok in toks:\n",
    "            if tok not in frequents:\n",
    "                frequents[tok]=0\n",
    "            frequents[tok]=frequents[tok]+1\n",
    "    sorts=sorted(frequents.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return frequents,sorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "smokFreqs,smokSorted=getFrequentTerms(smoking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smokFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smoke', 59),\n",
       " ('smoking', 48),\n",
       " ('be', 19),\n",
       " ('club', 18),\n",
       " ('stranger', 16),\n",
       " ('area', 16),\n",
       " ('h-', 16),\n",
       " ('https://t.co/rodp8rc9r7', 16),\n",
       " ('stop', 16),\n",
       " ('', 15),\n",
       " ('not', 12),\n",
       " ('blunt', 10),\n",
       " ('obama', 10),\n",
       " (\"'s\", 10),\n",
       " ('name', 10),\n",
       " ('fbi', 10),\n",
       " ('documents', 10),\n",
       " ('people', 10),\n",
       " ('will', 8),\n",
       " ('wyd', 7)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokSorted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vapFreqs,vapSorted = getFrequentTerms(vaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vap', 57),\n",
       " ('not', 17),\n",
       " ('#vap', 15),\n",
       " ('council', 13),\n",
       " ('staff', 13),\n",
       " ('work', 13),\n",
       " ('#vape', 13),\n",
       " ('smoking', 12),\n",
       " ('smoke', 12),\n",
       " ('vaping', 12),\n",
       " ('ban', 11),\n",
       " ('hour', 11),\n",
       " ('be', 11),\n",
       " ('anywhere', 10),\n",
       " ('s', 10),\n",
       " ('#vapelife', 9),\n",
       " ('bathroom', 9),\n",
       " ('premise', 8),\n",
       " ('#vaping', 8),\n",
       " ('#vapefam', 8)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vapSorted[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go through these lists to get an idea of some of the commonalities. One way to do this would be to create a new list containing all of the terms found in both lists, along with their counts for each list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smoke', 59, 12),\n",
       " ('need', 2, 1),\n",
       " ('drink', 5, 2),\n",
       " ('break', 1, 5),\n",
       " ('dude', 1, 1),\n",
       " ('smoking', 48, 12),\n",
       " ('2', 1, 1),\n",
       " ('pack', 1, 1),\n",
       " ('day', 4, 1),\n",
       " ('what', 4, 5),\n",
       " ('wish', 3, 1),\n",
       " ('play', 2, 1),\n",
       " ('', 7, 1),\n",
       " ('not', 12, 17),\n",
       " ('lol', 5, 2),\n",
       " ('amp', 2, 5),\n",
       " ('girl', 3, 1),\n",
       " ('feel', 3, 1),\n",
       " ('#smok', 2, 6),\n",
       " ('life', 1, 2),\n",
       " ('say', 7, 5),\n",
       " ('stop', 16, 4),\n",
       " ('love', 2, 5),\n",
       " ('be', 19, 11),\n",
       " ('try', 4, 5),\n",
       " ('quit', 7, 2),\n",
       " ('nicotine', 2, 2),\n",
       " (\"'s\", 10, 1),\n",
       " ('official', 5, 1),\n",
       " ('to', 6, 1),\n",
       " ('time', 5, 3),\n",
       " ('come', 1, 2),\n",
       " ('', 3, 2),\n",
       " ('people', 10, 2),\n",
       " ('ass', 7, 1),\n",
       " ('/', 3, 6),\n",
       " ('cigarette', 3, 4),\n",
       " ('shit', 1, 1),\n",
       " ('sleep', 1, 1),\n",
       " ('car', 1, 2),\n",
       " ('vape', 1, 8),\n",
       " ('like', 1, 3),\n",
       " ('@youtube', 1, 2),\n",
       " ('video', 1, 2),\n",
       " ('4', 1, 3),\n",
       " ('know', 5, 4),\n",
       " ('call', 1, 4),\n",
       " ('drug', 1, 2),\n",
       " ('go', 5, 5),\n",
       " ('kinda', 1, 1),\n",
       " ('way', 1, 2),\n",
       " ('', 15, 2),\n",
       " ('get', 3, 3),\n",
       " ('let', 3, 2),\n",
       " ('yo', 3, 2),\n",
       " ('marijuana', 2, 1),\n",
       " ('cool', 2, 1),\n",
       " ('nigga', 1, 1),\n",
       " ('use', 1, 3),\n",
       " ('look', 1, 6),\n",
       " ('ugly', 1, 1),\n",
       " ('unhealthy', 1, 1),\n",
       " ('here', 1, 2),\n",
       " ('better', 1, 2),\n",
       " ('', 1, 1),\n",
       " ('s', 2, 10),\n",
       " ('threat', 1, 2),\n",
       " ('order', 1, 2),\n",
       " ('juul', 1, 1),\n",
       " ('think', 1, 5)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged=[]\n",
    "for w,count in smokFreqs.items():\n",
    "    if w in vapFreqs:\n",
    "        vcount=vapFreqs[w]\n",
    "        item= (w,count,vcount)\n",
    "        merged.append(item)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these lists, a few observations come to mind:\n",
    "\n",
    "1. There are significant repeats, particularly in the top 20 terms.\n",
    "\n",
    "2. Frequent occurrences of terms like 'cigarette' in both list suggest that distinguishing between the two sets of tweets might be difficult.\n",
    "\n",
    "3. The vaping datasset contains many similar frequent terms like 'vap','vaping', 'vapor'. These similarities are also seen in related hashtags - `#vape`, `#vaping`, `#vapelife`, `#vapor`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3\n",
    "\n",
    "Based on these observations of the frequent terms, we will consider some of the questions raised in exercise 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 Exercise 3.1 and the `getFrequentTerms` method developed above take the relevant tokens from each tweet to be only those that meet certain criteria. However, we do not separately include the named entities.  One possible improvement would be to add any named entities to the list of tokens to be included. Revise `filterTweetTokens` to add any named entities to the end of the token list.  \n",
    "\n",
    "*Note* - be sure to add entities to the list only if they have a length of greater than zero! \n",
    "\n",
    "Try the result on a few tokens.  Do you see any potential problems with the simple approach to doing this? Does this strategy seem worth pursuing?\n",
    "\n",
    "3.3.2 We noticed that there are many repeated patterns in the tweets for vaping, including many terms and hashtags prefixed with 'vap'. One possible approach to this would be to further revise the lemmatizer to reduce these entries to common forms - perhaps 'vape' and '#vape'. Revise the `getTwitterNLP` routine above to include a lemmatizer that handles these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS - insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def includeToken(parsed):\n",
    "    tokens=[]\n",
    "    for tok in parsed:\n",
    "        if tok.is_stop==False and tok.pos_!='CCONJ' and tok.pos_!='ADP' and tok.pos_!='DET' and tok.lemma_!='-PRON-':\n",
    "            if tok.pos_=='PUNCT' or (tok.pos_=='PROPN' and tok.is_alpha==False):\n",
    "                if len(tok.lemma_.strip())>4 or (tok.pos_!='PUNCT' and len(tok.lemma_.strip())>3):\n",
    "                    tokens.append(tok.lemma_.strip())\n",
    "            else:\n",
    "                    tokens.append(tok.lemma_.strip())\n",
    "                    \n",
    "    for ent in parsed.ents:\n",
    "        if len(ent.text)>1:\n",
    "            tokens.append(ent.text);\n",
    "    return tokens;\n",
    "\n",
    "def filterTweetTokens(tweets):\n",
    "    tokens=[]\n",
    "    text=getRandomTweetText(tweets);\n",
    "    parsed=nlp(text)\n",
    "    tokens=includeToken(parsed);\n",
    "    while tokens.count('')!=0:\n",
    "            tokens.remove('');\n",
    "    return tokens,parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Smoking killa</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['smoking', 'killa']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "tokens,parsed=filterTweetTokens(smoking);\n",
    "displacy.render(docs=[parsed],jupyter=True, style='ent')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I liked a \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    @YouTube\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " video https://t.co/14YfaBhHo9 SMOKING A FAT \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    4\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "K IN THE NEW CRIB!!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " '@youtube',\n",
       " 'video',\n",
       " 'https://t.co/14yfabhho9',\n",
       " 'smoke',\n",
       " 'fat',\n",
       " '4',\n",
       " 'k',\n",
       " 'new',\n",
       " 'crib',\n",
       " '@YouTube']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "tokens,parsed=filterTweetTokens(smoking);\n",
    "displacy.render(docs=[parsed],jupyter=True, style='ent')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">All Aboard the Smoking Hot Trump Rocket</br>FB &amp; \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    RT Awesome Patriots\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "@tme42va\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "@NJ_2_FL\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "@hredriders\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "@hotpotatoe1\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    @krisiannc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "@briantopping66\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       "@currie14_kelly\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "@GarBear4Trump\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "@dlegend53\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "@JamesJdauto\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "@The_Zooniverse_\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "@ToniWilliams10\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "@treed_cat\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    @AnnThacker2\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "@karlacny\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    @mithmar\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "@MollyV178 https://t.co/H50Z8KNXfr</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['smoking',\n",
       " 'hot',\n",
       " 'trump',\n",
       " 'rocket',\n",
       " 'fb',\n",
       " 'amp',\n",
       " 'rt',\n",
       " 'awesome',\n",
       " 'patriots',\n",
       " '@tme42va',\n",
       " '@nj_2_fl',\n",
       " '@hredrider',\n",
       " '@hotpotatoe1',\n",
       " '@krisiannc',\n",
       " '@briantopping66',\n",
       " '@currie14_kelly',\n",
       " '@garbear4trump',\n",
       " '@dlegend53',\n",
       " '@jamesjdauto',\n",
       " '@the_zooniverse',\n",
       " '_',\n",
       " '@toniwilliams10',\n",
       " '@treed_cat',\n",
       " '@annthacker2',\n",
       " '@karlacny',\n",
       " '@mithmar',\n",
       " '@mollyv178',\n",
       " 'https://t.co/h50z8knxfr',\n",
       " 'RT Awesome Patriots\\n',\n",
       " '@krisiannc',\n",
       " '\\n@briantopping66\\n',\n",
       " '\\n@JamesJdauto\\n',\n",
       " '@AnnThacker2\\n',\n",
       " '@mithmar\\n']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "tokens,parsed=filterTweetTokens(smoking);\n",
    "displacy.render(docs=[parsed],jupyter=True, style='ent')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential problems: When the ORTH of an entity is same as its lemma, it will be counted twice when we are doing frequency analysis. \n",
    "\n",
    "Possible solution: Define a new array to store entities.\n",
    "\n",
    "Revised functions for this solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetTokens(parsed):\n",
    "    tokens=[]\n",
    "    entity=[]\n",
    "    for tok in parsed:\n",
    "        if tok.is_stop==False and tok.pos_!='CCONJ' and tok.pos_!='ADP' and tok.pos_!='DET' and tok.lemma_!='-PRON-':\n",
    "            if tok.pos_=='PUNCT' or (tok.pos_=='PROPN' and tok.is_alpha==False):\n",
    "                if len(tok.lemma_.strip())>3 or (tok.pos_!='PUNCT' and len(tok.lemma_.strip())>2):\n",
    "                    tokens.append(tok.lemma_.strip())\n",
    "            else:\n",
    "                    tokens.append(tok.lemma_.strip()) \n",
    "    \n",
    "    for ent in parsed.ents:\n",
    "        if len(ent.text)>1:\n",
    "            entity.append(ent.text);\n",
    "    while tokens.count('')!=0:\n",
    "            tokens.remove('');\n",
    "    return tokens,entity\n",
    "\n",
    "def getFrequentTerms(tweets,filtered=True,type=\"token\"):\n",
    "    frequents={}\n",
    "    for id in tweets.getIds():\n",
    "        text = tweets.getText(id)\n",
    "        parsed=nlp(text)\n",
    "        if filtered ==True:\n",
    "            toks,ents = filterTweetTokens(parsed)\n",
    "        else:\n",
    "            toks = [tok for tok in parsed]\n",
    "        \n",
    "        if type==\"entity\":\n",
    "            toks=ents\n",
    "        \n",
    "        for tok in toks:\n",
    "            if tok not in frequents:\n",
    "                frequents[tok]=0\n",
    "            frequents[tok]=frequents[tok]+1\n",
    "    sorts=sorted(frequents.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return frequents,sorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smoke', 58),\n",
       " ('smoking', 44),\n",
       " ('be', 22),\n",
       " ('will', 16),\n",
       " ('quit', 13),\n",
       " ('club', 13),\n",
       " ('stop', 13),\n",
       " ('stranger', 12),\n",
       " ('area', 12),\n",
       " ('h-', 12),\n",
       " ('https://t.co/rodp8rc9r7', 12),\n",
       " ('blunt', 10),\n",
       " ('try', 8),\n",
       " ('day', 8),\n",
       " ('lot', 7),\n",
       " ('amp', 7),\n",
       " ('people', 7),\n",
       " ('3', 6),\n",
       " ('parking', 6),\n",
       " ('clock', 6)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokFreqs,smokSorted=getFrequentTerms(smoking);\n",
    "smokSorted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FBI', 6),\n",
       " ('3 days', 5),\n",
       " ('Obama', 3),\n",
       " ('OBAMA', 3),\n",
       " ('NAME', 3),\n",
       " ('Another Senior Government Official', 3),\n",
       " ('today', 3),\n",
       " ('@smoking_reedus', 3),\n",
       " ('Sharpton', 3),\n",
       " ('a few miles', 3),\n",
       " ('Ferguson', 3),\n",
       " ('Canada', 2),\n",
       " ('California', 2),\n",
       " ('@TwdJvickydiane', 2),\n",
       " ('@YouTube', 2),\n",
       " ('\\u200d', 2),\n",
       " ('almost 2019', 2),\n",
       " ('9am', 1),\n",
       " ('5 weeks ago', 1),\n",
       " ('1 a month', 1)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokFreqs,smokSorted=getFrequentTerms(smoking,True,\"entity\");\n",
    "smokSorted[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP1():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "        \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "        \n",
    "    lm1 = [{ORTH: u'vape', LEMMA: u'vape', POS: u'VERB'}]\n",
    "    lm2 = [{ORTH: u'#vape', LEMMA: u'#vape', POS: u'VERB'}]\n",
    "    lm3 = [{ORTH: u'#vapelife', LEMMA: u'#vapelife', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'vap', lm1)\n",
    "    nlp.tokenizer.add_special_case(u'vaping', lm1)\n",
    "    nlp.tokenizer.add_special_case(u'#vap', lm2)\n",
    "    nlp.tokenizer.add_special_case(u'#vaping', lm2)\n",
    "    nlp.tokenizer.add_special_case(u'#vapelyfe', lm3)\n",
    "    \n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        while nbor.text=='#':\n",
    "                            nbor = nbor.nbor()\n",
    "                            token=token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp\n",
    "\n",
    "def filterTweetTokens(parsed):\n",
    "    tokens=[]\n",
    "    for tok in parsed:\n",
    "        if tok.is_stop==False and tok.pos_!='CCONJ' and tok.pos_!='ADP' and tok.pos_!='DET' and tok.lemma_!='-PRON-':\n",
    "            if tok.pos_=='PUNCT' or (tok.pos_=='PROPN' and tok.is_alpha==False):\n",
    "                if len(tok.lemma_.strip())>3 or (tok.pos_!='PUNCT' and len(tok.lemma_.strip())>2):\n",
    "                    tokens.append(tok.lemma_.strip())\n",
    "            else:\n",
    "                    tokens.append(tok.lemma_.strip())\n",
    "    while tokens.count('')!=0:\n",
    "            tokens.remove('');\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vape', '#vape', 'vape', '#vapelife', '#vape']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=getTwitterNLP1()\n",
    "tokens=filterTweetTokens(nlp(\"vap #vap vaping #vapelyfe #vaping\"))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END OF ANSWER cut above here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Final Notes\n",
    "\n",
    "Now that you've seen the basics of using natural language processing to extract understanding from the tweets, you're ready to move on to the next step.  [Part 4](SocialMedia%20-%20Part%204.ipynb) will take the results of the NLP output and create basic classifier machine learning models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
